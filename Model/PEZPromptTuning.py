import torch
from Model.t5_soft_prompt_tuning import SoftPromptTuning
import torch.nn.functional as F
import numpy as np
import umap
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics.pairwise import cosine_similarity as sk_cosine_similarity
import os
import json
import csv

class PEZPromptTuning(SoftPromptTuning):
    def __init__(self, model_name: str, n_tokens: int, prefix_len: int, learning_rate: float, train_dataloader, device):
        super(PEZPromptTuning, self).__init__(model_name, n_tokens, prefix_len, train_dataloader, device)
        self.optimizer = torch.optim.Adam([self.soft_prompts], lr=learning_rate)
        self.discrete_prompt_history = []  # ç”¨æ–¼è¨˜éŒ„æ¯æ¬¡é›¢æ•£åŒ–çš„è©å½™ç´¢å¼•
        
        self.training_mode = "vanilla"
        self.semantic_loss_weight = 0
        self.projection_start_epoch = 5
        
    def custom_forward(self, input_ids, labels=None, attention_mask=None, epoch=None, global_semantic_center=None, device=None):
        """
        è‡ªå®šç¾©å‰å‘å‚³æ’­ï¼Œåƒ…è¿”å›æå¤±ä»¥ä¾¿åœ¨ train_model ä¸­é€²è¡Œæ¢¯åº¦è¨ˆç®—ã€‚
        """
        # embedding_matrix = self.model.shared.weight  # è©åµŒå…¥çŸ©é™£
        # discrete_prompts = self.project_soft_prompt_to_discrete_space(embedding_matrix)
        # soft_prompts_backup = self.soft_prompts.data.clone()  # æš«å­˜é€£çºŒåµŒå…¥

        # self.soft_prompts.data = discrete_prompts.data  # æ›¿æ›ç‚ºé›¢æ•£åµŒå…¥
        outputs = self.forward_with_prompt(input_ids, labels, attention_mask, epoch, global_semantic_center, device)

        # self.soft_prompts.data = soft_prompts_backup  # æ¢å¾©é€£çºŒåµŒå…¥
        return outputs

    def finalize_discrete_prompts(self, train_loader, device, soft_prompt_after_projection_history, embedding_matrix, index=None,raw_texts=None, global_semantic_center=None, epoch=None):
        """
        åŸºæ–¼é›¢æ•£åŒ–çš„ P' è¨ˆç®—æå¤±ï¼Œä¸¦ä½¿ç”¨æ¢¯åº¦æ›´æ–°é€£çºŒåµŒå…¥ Pï¼Œç¢ºä¿æŠ•å½±æ–¹å‘èˆ‡å…ˆå‰æ›´æ–°æ–¹å‘ä¸€è‡´ã€‚
        Args:
            train_loader: è¨“ç·´æ•¸æ“šåŠ è¼‰å™¨
            device: è¨“ç·´è¨­å‚™
            soft_prompt_history: ä¿å­˜çš„ soft prompt æ­·å²
            threshold: æ–¹å‘ä¸€è‡´æ€§çš„é–¾å€¼
        """
        self.train()  # è¨­ç½®ç‚ºè¨“ç·´æ¨¡å¼
        total_loss = 0
        # å„²å­˜ç•¶å‰epochæ‰€æœ‰çš„æŠ•å½±åµŒå…¥
        current_projection_embeddings = []
        
        for batch in train_loader:
            input_ids = batch["input_ids"].to(device)
            labels = batch["labels"].to(device)

            # 1. æš«å­˜é€£çºŒåµŒå…¥ P
            soft_prompts_backup = self.soft_prompts.data.clone()
            
            # 2. è¨ˆç®— P'ï¼ˆé›¢æ•£åŒ–æŠ•å½±ï¼‰
            discrete_prompts = self.project_soft_prompt(self.soft_prompts, index, embedding_matrix, device)
            # embedding_matrix = embedding_matrix.to(device)
            # discrete_prompts = self.project_soft_prompt_to_discrete_space(embedding_matrix)
    
            # # **1.1 è¨ˆç®— batch å…§ token çš„ pairwise ç›¸ä¼¼åº¦**
            # similarity_matrix = self.compute_pairwise_similarity(discrete_prompts.cpu().numpy())
            
            # # **1.2 æ‰¾å‡ºéæ–¼åˆ†æ•£çš„ token**
            # low_similarity_tokens = self.should_replace_token(similarity_matrix)
            
            # # **1.3 è¨ˆç®— batch èªç¾©ä¸­å¿ƒ**
            # batch_center = self.compute_batch_center(discrete_prompts.cpu().numpy())

            # # **1.4 ä¿®æ­£éæ–¼åˆ†æ•£çš„ token**
            # if len(low_similarity_tokens) > 0:
            #     nearest_neighbors = self.find_nearest_neighbors(discrete_prompts.cpu().numpy(), embedding_matrix.cpu().numpy())
            #     for idx in low_similarity_tokens:
            #         discrete_prompts[idx] = torch.tensor(
            #             self.smooth_token_adjustment(discrete_prompts[idx].cpu().numpy(),
            #                                     embedding_matrix[nearest_neighbors[idx][0]].cpu().numpy(),
            #                                     batch_center)
            #         ).to(device)

            # 3. è¨˜éŒ„æŠ•å½±åµŒå…¥**
            current_projection_embeddings.append(discrete_prompts.clone().detach().cpu())

            # 4. æ›¿æ›ç‚ºé›¢æ•£åµŒå…¥ P' é€²è¡Œå‰å‘å‚³æ’­
            self.soft_prompts.data = discrete_prompts.data
            
            # 5. è¨ˆç®—æå¤±
            self.optimizer.zero_grad()
            outputs = self.forward_with_prompt(input_ids, labels=labels, attention_mask=batch['attention_mask'], epoch=epoch, global_semantic_center=global_semantic_center, device=device)
            loss = outputs.loss

            # 6. æ¢å¾©åŸå§‹é€£çºŒåµŒå…¥ P
            self.soft_prompts.data = soft_prompts_backup

            # 7. åŸºæ–¼æå¤±æ›´æ–°é€£çºŒåµŒå…¥ P
            loss.backward()
            self.optimizer.step()
            total_loss += loss.item()
            
        # average_embedding = self.analyze_embedding_distribution(current_projection_embeddings)
        # soft_prompt_after_projection_history.append(torch.tensor(average_embedding))
        # chosen_index = self.get_top1_nearest_text(average_embedding, embedding_matrix, raw_texts)
        # top1_neighbors_texts = self.get_topk_neighbors_for_top1(chosen_index, embedding_matrix, raw_texts, k=5)
        # self.save_epoch_neighbors(top1_neighbors_texts)
              
        # # **ğŸ”¹ å°å‡º Top-1 æœ€è¿‘é„°çš„ Top-5 è¿‘é„°æ–‡æœ¬**
        # print("\nğŸ”¹ ç•¶å‰ Epoch çš„ Top-1 æœ€è¿‘é„°çš„ Top-5 è¿‘é„°æ–‡æœ¬:")
        # for idx, text in enumerate(top1_neighbors_texts, 1):
        #     print(f"  {idx}. {text}")
        
        # print(f"Finalized with Loss: {total_loss / len(train_loader):.4f}")

        
    def save_epoch_neighbors(self, neighbors, file_path="epoch_texts/all_epochs_neighbors.json"):
        """
        å„²å­˜æ‰€æœ‰ Epoch ç”¢ç”Ÿçš„ Top-5 è¿‘é„°æ–‡æœ¬ï¼Œä¸¦è‡ªå‹•ç´¯ç©æ–° Epoch çš„çµæœã€‚
        å¦‚æœç›®éŒ„ä¸å­˜åœ¨ï¼Œå‰‡è‡ªå‹•å‰µå»ºã€‚

        Parameters:
            neighbors (list): ç•¶å‰ Epoch ç”¢ç”Ÿçš„ Top-5 é„°è¿‘æ–‡æœ¬ (list of strings)ã€‚
            file_path (str): å­˜å„²é„°è¿‘æ–‡æœ¬çš„ JSON æª”æ¡ˆè·¯å¾‘ã€‚
        """
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨ï¼Œå¦å‰‡å‰µå»º
        dir_path = os.path.dirname(file_path)
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)  # è‡ªå‹•å‰µå»ºç›®éŒ„

        # å¦‚æœæª”æ¡ˆå·²å­˜åœ¨ï¼Œå‰‡è®€å–ç¾æœ‰å…§å®¹
        if os.path.exists(file_path):
            with open(file_path, "r", encoding="utf-8") as f:
                try:
                    data = json.load(f)
                except json.JSONDecodeError:
                    data = []  # å¦‚æœè®€å–å¤±æ•—ï¼Œå‰‡åˆå§‹åŒ–ç‚ºç©ºåˆ—è¡¨
        else:
            data = []  # å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ï¼Œå‰‡åˆå§‹åŒ–ç‚ºç©ºåˆ—è¡¨

        # è¿½åŠ ç•¶å‰ Epoch çš„é„°è¿‘æ–‡æœ¬
        data.append({"top5_neighbors": neighbors})

        # å­˜å› JSON æª”æ¡ˆ
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)

        print(f"âœ… å·²å­˜å„² {len(data)} å€‹ Epoch çš„é„°è¿‘æ–‡æœ¬è‡³ {file_path}")

    def log_prompt_shift(self, epoch, p_before, p_after, path="epoch_prompt_shift.csv"):
        p_before = p_before.view(-1)
        p_after = p_after.view(-1)
        cos_sim = F.cosine_similarity(p_before.unsqueeze(0), p_after.unsqueeze(0), dim=1)
        cos_dist = 1 - cos_sim.item()

        file_exists = os.path.exists(path)
        with open(path, "a", newline="") as f:
            writer = csv.writer(f)
            if not file_exists:
                writer.writerow(["epoch", "cosine_distance_between_tuning_and_projected"])
            writer.writerow([epoch, cos_dist])
        print(f"[Epoch {epoch}] Cosine Distance (P_tuning vs P_projected): {cos_dist:.4f}")


    def get_top1_nearest_text(self, average_embedding, embedding_matrix, raw_texts):
        """
        å–å¾— Soft Prompt ç•¶å‰æœ€æ¥è¿‘çš„æ–‡æœ¬ï¼ˆTop-1 æœ€è¿‘é„°ï¼‰ã€‚
        
        Parameters:
            average_embedding (numpy.ndarray): Soft Prompt çš„å¹³å‡æŠ•å½±åµŒå…¥
            embedding_matrix (numpy.ndarray): è¨“ç·´æ•¸æ“šåµŒå…¥çŸ©é™£
            raw_texts (list): è¨“ç·´æ•¸æ“šçš„æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            str: æœ€æ¥è¿‘çš„æ–‡æœ¬
        """
        # ç¢ºä¿ `embedding_matrix` è½‰æ›ç‚º NumPy
        if isinstance(embedding_matrix, torch.Tensor):
            embedding_matrix = embedding_matrix.cpu().numpy()  # ç§»å‹•åˆ° CPUï¼Œè½‰æ›ç‚º numpy
        
        if isinstance(average_embedding, torch.Tensor):
            average_embedding = average_embedding.cpu().numpy().reshape(1, -1)  # ç¢ºä¿ç‚º NumPy é™£åˆ—
            
        similarities = sk_cosine_similarity(average_embedding, embedding_matrix)[0]
        top1_index = np.argmax(similarities)  # å–æœ€æ¥è¿‘çš„ç´¢å¼•
        top1_similarity = similarities[top1_index]
        
        print(f"ğŸ”¹ Top-1 æœ€è¿‘é„°ç›¸ä¼¼åº¦: {top1_similarity:.4f}")
        return top1_index

    def get_topk_neighbors_for_top1(self, top1_index, embedding_matrix, raw_texts, k=5):
        """
        å–å¾— Top-1 æœ€è¿‘é„°çš„ Top-K è¿‘é„°æ–‡æœ¬ã€‚
        
        Parameters:
            top1_index (int): Top-1 æœ€è¿‘é„°çš„ç´¢å¼•
            embedding_matrix (numpy.ndarray): è¨“ç·´æ•¸æ“šçš„åµŒå…¥çŸ©é™£
            raw_texts (list): è¨“ç·´æ•¸æ“šçš„æ–‡æœ¬åˆ—è¡¨
            k (int): å–å‰ K å€‹æœ€è¿‘é„°
            
        Returns:
            list: K å€‹æœ€æ¥è¿‘çš„æ–‡æœ¬
        """
        if isinstance(embedding_matrix, torch.Tensor):
            embedding_matrix = embedding_matrix.cpu().numpy()  # ç§»å‹•åˆ° CPUï¼Œè½‰æ›ç‚º numpy
            
        top1_embedding = embedding_matrix[top1_index].reshape(1, -1)
        similarities = sk_cosine_similarity(top1_embedding, embedding_matrix)[0]
        top_k_indices = np.argsort(similarities)[-k:][::-1]  # å–æœ€ç›¸ä¼¼çš„ K å€‹ç´¢å¼•
        return [raw_texts[i] for i in top_k_indices]

    def compute_pairwise_similarity(self, projection_embeddings):
        """
        è¨ˆç®— batch å…§æ‰€æœ‰ token ä¹‹é–“çš„ç›¸ä¼¼åº¦ã€‚

        Args:
            projection_embeddings (numpy.ndarray): ç•¶å‰ batch å…§çš„æŠ•å½±åµŒå…¥ [n_tokens, embedding_dim]

        Returns:
            numpy.ndarray: [n_tokens, n_tokens] å½¢ç‹€çš„ pairwise similarity matrix
        """
        return cosine_similarity(projection_embeddings)

    def should_replace_token(self, pairwise_similarities, min_threshold=0.5, max_threshold=0.9):
        """
        æ ¹æ“š token ä¹‹é–“çš„ç›¸ä¼¼åº¦ï¼Œæ±ºå®šæ˜¯å¦éœ€è¦æ›¿æ› tokenã€‚
        
        Args:
            pairwise_similarities (numpy.ndarray): [n_tokens, n_tokens] å½¢ç‹€çš„ç›¸ä¼¼åº¦çŸ©é™£
            min_threshold (float): å…è¨±çš„æœ€ä½ç›¸ä¼¼åº¦ï¼Œä½æ–¼æ­¤å€¼çš„ token å¯èƒ½éœ€è¦èª¿æ•´
            max_threshold (float): å…è¨±çš„æœ€é«˜ç›¸ä¼¼åº¦ï¼Œè¶…éæ­¤å€¼çš„ token ä¸éœ€è¦èª¿æ•´
        
        Returns:
            list: éœ€è¦é€²è¡Œæ›¿æ›çš„ token ç´¢å¼•
        """
        mean_similarities = np.mean(pairwise_similarities, axis=1)  # è¨ˆç®—æ¯å€‹ token çš„å¹³å‡ç›¸ä¼¼åº¦
        replace_tokens = np.where(mean_similarities < min_threshold)[0]  # æ‰¾å‡ºéæ–¼åˆ†æ•£çš„ token
        return replace_tokens

    def compute_batch_center(self, projection_embeddings):
        """
        è¨ˆç®— batch å…§æ‰€æœ‰ token çš„èªç¾©ä¸­å¿ƒé»ï¼ˆå¹³å‡åµŒå…¥ï¼‰ã€‚
        
        Args:
            projection_embeddings (numpy.ndarray): ç•¶å‰ batch å…§çš„æŠ•å½±åµŒå…¥ [n_tokens, embedding_dim]

        Returns:
            numpy.ndarray: èªç¾©ä¸­å¿ƒé» [1, embedding_dim]
        """
        return np.mean(projection_embeddings, axis=0)

    def find_nearest_neighbors(self, projection_embeddings, embedding_space, k=5):
        """
        å° batch å…§æ¯å€‹ token æ‰¾æœ€è¿‘é„°çš„åµŒå…¥å‘é‡ã€‚

        Args:
            projection_embeddings (numpy.ndarray): ç•¶å‰ batch å…§çš„æŠ•å½±åµŒå…¥ï¼Œå½¢ç‹€ç‚º [n_tokens, embedding_dim]
            embedding_space (numpy.ndarray): è¨“ç·´æ¨£æœ¬æ§‹æˆçš„åµŒå…¥ç©ºé–“ [n_samples, embedding_dim]
            k (int): è¿‘é„°æ•¸é‡

        Returns:
            list: æ¯å€‹ token åœ¨åµŒå…¥ç©ºé–“æ‰¾åˆ°çš„æœ€è¿‘é„°ç´¢å¼•
        """
        neigh = NearestNeighbors(n_neighbors=k, metric="cosine")
        neigh.fit(embedding_space)  # è¨“ç·´æ¨£æœ¬ä½œç‚ºåŸºæº–
        
        distances, indices = neigh.kneighbors(projection_embeddings)
        
        return indices  # å›å‚³æ¯å€‹ token åœ¨åµŒå…¥ç©ºé–“çš„ k è¿‘é„°ç´¢å¼•

    def smooth_token_adjustment(self, projection_embedding, nearest_embedding, batch_center, alpha=0.3):
        """
        è®“ token å‘æœ€è¿‘é„°è©åµŒå…¥é è¿‘ä¸€é»ï¼Œè€Œä¸æ˜¯å®Œå…¨è®Šæˆæœ€è¿‘é„°ï¼Œä»¥ä¿ç•™èªç¾©å¤šæ¨£æ€§ã€‚

        Args:
            projection_embedding (numpy.ndarray): åŸ token çš„åµŒå…¥ [1, embedding_dim]
            nearest_embedding (numpy.ndarray): æœ€è¿‘é„°è©åµŒå…¥ [1, embedding_dim]
            batch_center (numpy.ndarray): batch å…§ token çš„èªç¾©ä¸­å¿ƒ [1, embedding_dim]
            alpha (float): æ§åˆ¶ token è®Šå‹•ç¨‹åº¦ (0~1)ï¼Œå€¼è¶Šå¤§ä»£è¡¨ token è®Šå‹•è¶Šå¤§

        Returns:
            numpy.ndarray: å¹³æ»‘èª¿æ•´å¾Œçš„ token åµŒå…¥
        """
        return alpha * nearest_embedding + (1 - alpha) * batch_center

    def find_tokens_far_from_center(self, projection_embeddings, batch_center, threshold=0.5):
        """
        æ‰¾å‡º batch å…§è·é›¢èªç¾©ä¸­å¿ƒéé çš„ tokenã€‚

        Args:
            projection_embeddings (numpy.ndarray): ç•¶å‰ batch å…§çš„æŠ•å½±åµŒå…¥ [n_tokens, embedding_dim]
            batch_center (numpy.ndarray): batch å…§ token çš„èªç¾©ä¸­å¿ƒ [1, embedding_dim]
            threshold (float): è¨­å®šçš„è·é›¢é–€æª»

        Returns:
            list: éœ€è¦é‡æ–°æ›¿æ›çš„ token ç´¢å¼•
        """
        distances = np.linalg.norm(projection_embeddings - batch_center, axis=1)  # è¨ˆç®—æ¯å€‹ token åˆ°èªç¾©ä¸­å¿ƒçš„è·é›¢
        far_tokens = np.where(distances > threshold)[0]  # æ‰¾å‡ºè·é›¢éé çš„ token
        return far_tokens

    def analyze_embedding_distribution(self, embeddings):
        """
        Analyze the distribution of embeddings to calculate:
        1. The average embedding.
        2. A single metric to determine whether the embeddings are sufficiently concentrated.

        Parameters:
            embeddings (numpy.ndarray): A 2D array of shape (n_samples, embedding_dim),
                                        where each row is an embedding vector.

        Returns:
            dict: A dictionary containing:
                - 'average_embedding': The mean embedding vector.
                - 'mean_distance': The average distance from each embedding to the average embedding.
        """
        # Ensure embeddings is a NumPy array
        embeddings = np.array(embeddings)
        
        # Calculate the average embedding
        average_embedding = np.mean(embeddings, axis=0)
    
        # Calculate distances from each embedding to the average embedding
        distances = np.linalg.norm(embeddings - average_embedding, axis=1)

        # Calculate mean distance
        mean_distance = np.mean(distances)
        # print("mean distance:", mean_distance)

        # Return results as a dictionary
        return average_embedding

    def get_nearest_neighbors(self, average_embedding, embedding_matrix, raw_texts, k=5):
        """
        æ ¹æ“šç•¶å‰ epoch çš„å¹³å‡æŠ•å½±åµŒå…¥ï¼Œæ‰¾åˆ°å…¶åœ¨èªç¾©ç©ºé–“ä¸­çš„æœ€è¿‘é„°æ¨£æœ¬ã€‚
        
        Parameters:
            average_embedding (numpy.ndarray): ç•¶å‰ epoch è¨ˆç®—å‡ºçš„å¹³å‡æŠ•å½±åµŒå…¥ï¼Œå½¢ç‹€ç‚º (embedding_dim,).
            embedding_matrix (numpy.ndarray): é å…ˆæ§‹å»ºçš„åµŒå…¥ç©ºé–“çŸ©é™£ï¼Œå½¢ç‹€ç‚º (n_samples, embedding_dim)ã€‚
            raw_texts (list): èˆ‡ embedding_matrix å°æ‡‰çš„åŸå§‹æ–‡æœ¬åˆ—è¡¨ï¼Œé•·åº¦ç‚º n_samplesã€‚
            k (int): éœ€è¦æª¢ç´¢çš„æœ€è¿‘é„°æ•¸é‡ã€‚

        Returns:
            list: åŒ…å« k å€‹æœ€æ¥è¿‘çš„åŸå§‹æ–‡æœ¬ï¼Œä»£è¡¨ soft prompt ç•¶å‰çš„èªç¾©å°æ‡‰æ–‡æœ¬ã€‚
        """
        # ç¢ºä¿ `embedding_matrix` è½‰æ›ç‚º NumPy
        if isinstance(embedding_matrix, torch.Tensor):
            embedding_matrix = embedding_matrix.cpu().numpy()  # ç§»å‹•åˆ° CPUï¼Œè½‰æ›ç‚º numpy
        
        if isinstance(average_embedding, torch.Tensor):
            average_embedding = average_embedding.cpu().numpy().reshape(1, -1)  # ç¢ºä¿ç‚º NumPy é™£åˆ—

        # è¨ˆç®— cosine ç›¸ä¼¼åº¦
        similarities = sk_cosine_similarity(average_embedding, embedding_matrix)[0]

        # å–æœ€ç›¸ä¼¼çš„ K å€‹ç´¢å¼•
        top_k_indices = np.argsort(similarities)[-k:][::-1]

        # ç²å–å°æ‡‰çš„æ–‡æœ¬å…§å®¹
        top_k_texts = [raw_texts[i] for i in top_k_indices]
        top_k_similarities = similarities[top_k_indices]

        # é¡¯ç¤ºçµæœ
        print("\nğŸ”¹ ç•¶å‰ epoch å¹³å‡æŠ•å½±åµŒå…¥çš„æœ€è¿‘é„°æ–‡æœ¬ï¼š")
        for i, (text, sim) in enumerate(zip(top_k_texts, top_k_similarities)):
            print(f"Top {i+1}: (Similarity: {sim:.4f}) {text}")

        return top_k_texts
    
    def compute_global_semantic_center(self, embedding_space: torch.Tensor) -> torch.Tensor:
        """
        è¨ˆç®—æ•´å€‹è¨“ç·´é›†çš„èªç¾©ä¸­å¿ƒï¼ˆå…¨åŸŸèªç¾©å¹³å‡åµŒå…¥ï¼‰ã€‚

        Args:
            embedding_space (torch.Tensor): shape (N, D)ï¼ŒåŒ…å«æ•´å€‹è¨“ç·´é›†çš„å¥å­ç´šåµŒå…¥å‘é‡ã€‚

        Returns:
            torch.Tensor: å…¨åŸŸèªç¾©ä¸­å¿ƒï¼Œshape (D,)
        """
        return embedding_space.mean(dim=0)  # å°æ‰€æœ‰å¥å­å¹³å‡