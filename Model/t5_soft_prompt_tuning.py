import os
import random
import time
import numpy as np
import torch
import torch.nn as nn
from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch.nn.functional as F
from Config.config import *
from sentence_transformers.util import (semantic_search, 
                                        dot_score, 
                                        normalize_embeddings)
class SoftPromptTuning(nn.Module):
    """
    Soft Prompt Tuning æ¨¡å‹
    """
    def __init__(self, model_name: str, n_tokens: int, prefix_len: int, train_dataloader, device):
        """
        åˆå§‹åŒ– SoftPromptTuning æ¨¡å‹
        Args:
            model_name (str): é è¨“ç·´æ¨¡å‹çš„åç¨± (e.g., "t5-base")
            n_tokens (int): å¼•å°çš„è™›æ“¬ token æ•¸é‡
            prefix_len (int): å¼•å°çš„å‰ç¶´é•·åº¦
        """
        super(SoftPromptTuning, self).__init__()
        self.model = T5ForConditionalGeneration.from_pretrained(model_name)
        self.tokenizer = T5Tokenizer.from_pretrained(model_name)
        self.n_tokens = n_tokens
        self.prefix_len = prefix_len
        
        # å¾è¨“ç·´è³‡æ–™åµŒå…¥ç©ºé–“åˆå§‹åŒ–
        # embedding_space = self.extract_and_cache_training_embeddings_with_context(self.model, train_dataloader, device)
        # self.soft_prompts = nn.Parameter(self.init_prompts_from_training_embeddings(embedding_space, self.n_tokens))
        
        #æ”¹é€²çš„åˆå§‹åŒ–ï¼Œå¾è©åµŒå…¥çŸ©é™£ä¸­é¸å–éš¨æ©ŸåµŒå…¥é€²è¡Œåˆå§‹åŒ–
        #ğŸš€ é€™è£¡ç”¨ torch.Generator() ç”¢ç”Ÿç¨ç«‹çš„éš¨æ©Ÿç‹€æ…‹
        # gen = torch.Generator()
        # gen.manual_seed(int(time.time() * 1000000) % 2**32)
        # self.soft_prompts = nn.Parameter(self.init_prompts_from_vocab(generator=gen))
        self.soft_prompts = nn.Parameter(self.init_prompts_from_vocab())
        
        # å®Œå…¨éš¨æ©Ÿåˆå§‹åŒ–
        # self.soft_prompts = nn.Parameter(self.init_prompts_random())
        
        
    def init_prompts_from_vocab(self, generator=None):
        """
        åˆå§‹åŒ– Prompt åµŒå…¥ï¼Œå¾æ¨¡å‹çš„è©åµŒå…¥çŸ©é™£ä¸­é¸å–éš¨æ©ŸåµŒå…¥ã€‚
        """
        vocab_size, embed_dim = self.model.shared.weight.shape
        # ä½¿ç”¨æŒ‡å®šçš„ Generator ä¾†ç¢ºä¿ soft prompt åˆå§‹åŒ–æ˜¯éš¨æ©Ÿçš„
        # random_indices = torch.randint(0, vocab_size, (self.n_tokens,), generator=generator)
        random_indices = torch.randint(0, vocab_size, (self.n_tokens,))
        initial_embeds = self.model.shared.weight[random_indices].detach().clone()
        return initial_embeds
    
    def init_prompts_far_from_embedding(self, embedding_space, scale_factor=5):
        """
        åŸºæ–¼ Cosine Similarity åˆå§‹åŒ–ä¸€å€‹é é›¢è¨“ç·´è³‡æ–™åµŒå…¥ç©ºé–“çš„ soft promptã€‚

        Args:
            embedding_space (torch.Tensor): è¨“ç·´è³‡æ–™çš„åµŒå…¥ç©ºé–“ (num_samples, embedding_dim)
            scale_factor (float): æ§åˆ¶é é›¢ç¨‹åº¦çš„ä¿‚æ•¸ (é è¨­ç‚º 5)

        Returns:
            torch.Tensor: è·é›¢åµŒå…¥ç©ºé–“è¶³å¤ é çš„ soft prompt
        """
        embed_mean = embedding_space.mean(dim=0)  # åµŒå…¥ç©ºé–“çš„å‡å€¼
        embed_std = embedding_space.std(dim=0)    # åµŒå…¥ç©ºé–“çš„æ¨™æº–å·®
        
        # **æ­¥é©Ÿ 1ï¼šéš¨æ©Ÿåˆå§‹åŒ– soft promptï¼Œä¸¦æ¨™æº–åŒ–ç‚ºå–®ä½å‘é‡**
        random_vectors = torch.randn(self.n_tokens, embedding_space.shape[1])  # éš¨æ©Ÿåˆå§‹åŒ–
        random_vectors = F.normalize(random_vectors, p=2, dim=1)  # L2 æ­£è¦åŒ–ï¼Œç¢ºä¿æ˜¯å–®ä½å‘é‡
        
        # **æ­¥é©Ÿ 2ï¼šè¨ˆç®—èˆ‡åµŒå…¥ç©ºé–“çš„ Cosine Similarity**
        cosine_sim = F.cosine_similarity(random_vectors.unsqueeze(1), embedding_space.unsqueeze(0), dim=2)  # (n_tokens, num_samples)
        min_cosine_sim, _ = cosine_sim.max(dim=1)  # å–èˆ‡åµŒå…¥ç©ºé–“ä¸­æœ€ç›¸ä¼¼çš„é»
        
        # **æ­¥é©Ÿ 3ï¼šèª¿æ•´é é›¢ç¨‹åº¦**
        while min_cosine_sim.max() > 0.2:  # ç¢ºä¿ Cosine Similarity ä½æ–¼ 0.2 (è·é›¢è¼ƒé )
            random_vectors += scale_factor * torch.randn_like(random_vectors)  # æ·»åŠ æ“¾å‹•
            random_vectors = F.normalize(random_vectors, p=2, dim=1)  # é‡æ–°æ­£è¦åŒ–
            cosine_sim = F.cosine_similarity(random_vectors.unsqueeze(1), embedding_space.unsqueeze(0), dim=2)
            min_cosine_sim, _ = cosine_sim.max(dim=1)

        return random_vectors
    
    def init_prompts_random(self):
        """
        åˆå§‹åŒ– Prompt åµŒå…¥ï¼Œä½¿å…¶ç‚ºå®Œå…¨éš¨æ©Ÿçš„æ•¸å€¼ï¼Œç„¡é—œæ–¼è©åµŒå…¥ç©ºé–“ã€‚
        """
        embed_dim = self.model.shared.weight.shape[1]
        random_embeddings = torch.randn(self.n_tokens, embed_dim)  # æ¨™æº–æ­£æ…‹åˆ†ä½ˆ N(0,1)
        return random_embeddings

    def forward_with_prompt(self, input_ids, labels=None, attention_mask=None, epoch=None, global_semantic_center=None, device=None):
        """
        å¸¶æœ‰ Prompt çš„å‰å‘å‚³æ’­æ–¹æ³•ï¼Œç”¨æ–¼ Soft Prompt Tuningã€‚
        Args:
            input_ids (torch.Tensor): è¼¸å…¥çš„ token IDã€‚
            attention_mask (torch.Tensor, optional): æ³¨æ„åŠ›é®ç½©ï¼Œé»˜èªç‚º Noneã€‚
            labels (torch.Tensor, optional): æ¨™ç±¤ token IDï¼Œé»˜èªç‚º Noneã€‚
        Returns:
            torch.Tensor: æ¨¡å‹çš„è¼¸å‡ºï¼ŒåŒ…æ‹¬æå¤±å’Œ logitsã€‚
        """
        self.current_epoch = epoch if epoch is not None else self.current_epoch
        # å‰µå»º soft prompt åµŒå…¥
        soft_prompts = self.soft_prompts.unsqueeze(0).expand(input_ids.size(0), -1, -1)
        # åŸå§‹åµŒå…¥
        inputs_embeds = self.model.shared(input_ids)
        # åˆä½µ soft prompts å’ŒåŸå§‹åµŒå…¥
        combined_embeds = torch.cat((soft_prompts, inputs_embeds), dim=1)

        # æ›´æ–° attention mask ä»¥é©æ‡‰æ–°åŠ å…¥çš„ soft prompts
        if attention_mask is not None:
            # extended_attention_mask = torch.cat(
            #     (torch.ones(soft_prompts.size()[:2], dtype=attention_mask.dtype).to(attention_mask.device),
            #      attention_mask),
            #     dim=1
            # )
            soft_prompt_mask = torch.ones(
                (soft_prompts.size(0), soft_prompts.size(1)),  # (batch, prompt_len)
                dtype=attention_mask.dtype,
                device=device
            )
            extended_attention_mask = torch.cat((soft_prompt_mask, attention_mask.to(device)), dim=1)  # (batch, prompt_len + seq_len)
        else:
            extended_attention_mask = None

        # fix: ç¢ºä¿é•·åº¦ä¸€è‡´
        assert combined_embeds.size(1) == extended_attention_mask.size(1), \
            f"Embeds len: {combined_embeds.size(1)}, Mask len: {extended_attention_mask.size(1)}"

        # å‰å‘å‚³æ’­
        if labels is not None:
            outputs = self.model(
                inputs_embeds=combined_embeds,
                attention_mask=extended_attention_mask,
                labels=labels
            )
        else:
            # è‡ªå‹•ç”Ÿæˆ batch_size å€‹ decoder_input_idsï¼Œéƒ½æ˜¯ <pad>ï¼ˆT5 é è¨­ decoder é–‹å§‹ tokenï¼‰
            decoder_start_token_id = self.model.config.decoder_start_token_id or self.tokenizer.pad_token_id
            decoder_input_ids = torch.full(
                (input_ids.size(0), 1),
                decoder_start_token_id,
                dtype=torch.long,
                device=input_ids.device
            )

            outputs = self.model(
                inputs_embeds=combined_embeds,
                attention_mask=extended_attention_mask,
                decoder_input_ids=decoder_input_ids
            )

        
        ce_loss = outputs.loss
        semantic_loss = torch.tensor(0.0).to(device)

        if self.training and getattr(self, "training_mode", "vanilla") in ["semantic_only", "two_stage"]:
            if self.training_mode == "semantic_only" or self.current_epoch < getattr(self, "projection_start_epoch", 5):
                semantic_loss = self.semantic_alignment_loss_cosine(
                    soft_prompts=soft_prompts,
                    global_semantic_center=global_semantic_center
                )
        
        lambda_sem = getattr(self, "semantic_loss_weight", 0.05)
        
        # ğŸ› ï¸ æ ¹æ“šæ˜¯å¦æœ‰ ce_lossï¼Œæ±ºå®š total_loss è¦ä¸è¦åˆä½µ
        if ce_loss is not None:
            total_loss = ce_loss + lambda_sem * semantic_loss
        else:
            total_loss = None
        # print(f"ce: {ce_loss.item()}, semantic: {semantic_loss.item()}, total: {total_loss.item()}")
        # 5. åŒ…è£å›å‚³æ ¼å¼
        outputs.loss = total_loss
        outputs.ce_loss = ce_loss
        outputs.semantic_loss = semantic_loss
        
        return outputs

    def semantic_alignment_loss_cosine(self, soft_prompts, global_semantic_center):
        """
        è¨ˆç®— soft prompt å‘é‡å¹³å‡å€¼èˆ‡èªç¾©ä¸­å¿ƒä¹‹é–“çš„ cosine lossã€‚
        Args:
            soft_prompts: (B, P, D)
            global_semantic_center: (D,)
        Returns:
            Cosine-based loss
        """
        device = soft_prompts.device
        prompt_mean = soft_prompts.mean(dim=1)  # (B, D)
        global_center = global_semantic_center.to(device).unsqueeze(0)  # (1, D)

        cosine_sim = F.cosine_similarity(prompt_mean, global_center.expand_as(prompt_mean), dim=-1)  # (B,)
        cosine_loss = 1 - cosine_sim  # è¶Šæ¥è¿‘ 1 è¡¨ç¤ºè¶Šå°é½Š â†’ loss è¶Šå°

        return cosine_loss.mean()

    def new_semantic_alignment_loss_global_center(self, soft_prompts, global_semantic_center):
        """
        æ”¹å¯«ç‰ˆæœ¬ï¼šå°‡ soft prompt å¹³å‡å€¼å°é½Šè‡³æ•´å€‹è¨“ç·´é›†çš„èªç¾©ä¸­å¿ƒã€‚
        
        Args:
            soft_prompts: shape (B, P, D)
            global_semantic_center: torch.Tensor, shape (D,)
        
        Returns:
            MSE loss between soft prompt mean and global semantic center
        """
        device = soft_prompts.device
        prompt_mean = soft_prompts.mean(dim=1)  # (B, D)
        global_center = global_semantic_center.to(device).unsqueeze(0)  # (1, D)

        return F.mse_loss(prompt_mean, global_center.expand_as(prompt_mean))

    def custom_forward(self, input_ids, labels=None):
        """
        è‡ªå®šç¾©å‰å‘å‚³æ’­ï¼Œé©ç”¨æ–¼å¤–éƒ¨è¨“ç·´æˆ–é©—è­‰æ–¹æ³•ã€‚
        """
        return self.forward_with_prompt(input_ids=input_ids, labels=labels)

    def save_soft_prompts(self, path="Checkpoints/soft_prompts.pt"):
        """
        ä¿å­˜ soft prompts åˆ°æŒ‡å®šè·¯å¾‘ã€‚
        Args:
            path (str): ä¿å­˜æ–‡ä»¶çš„è·¯å¾‘ï¼Œé»˜èªç‚º "Checkpoints/soft_prompts.pt"ã€‚
        """
        dir_name = os.path.dirname(path)
        if dir_name and not os.path.exists(dir_name):
            os.makedirs(dir_name)
        
        torch.save(self.soft_prompts.data, path)
        print(f"Soft prompts saved to {path}")

    def load_soft_prompts(self, path="Checkpoints/soft_prompts.pt"):
        """
        åŠ è¼‰ soft prompts æ–‡ä»¶ã€‚
        Args:
            path (str): åŠ è¼‰æ–‡ä»¶çš„è·¯å¾‘ï¼Œé»˜èªç‚º "Checkpoints/soft_prompts.pt"ã€‚
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Checkpoint file not found at {path}")
        
        self.soft_prompts.data = torch.load(path).to(self.soft_prompts.device)
        print(f"Soft prompts loaded from {path}")


    def project_soft_prompt_to_discrete_space(self, embedding_matrix):
        """
        å°‡é€£çºŒ prompt æŠ•å½±åˆ°é›¢æ•£è©åµŒå…¥ç©ºé–“ã€‚
        Args:
            embedding_matrix (torch.Tensor): è©åµŒå…¥çŸ©é™£ã€‚
        Returns:
            torch.Tensor: æŠ•å½±å¾Œçš„ prompt åµŒå…¥ã€‚
        """
        prompt_norm = F.normalize(self.soft_prompts, p=2, dim=-1)
        embedding_norm = F.normalize(embedding_matrix, p=2, dim=-1)
        similarity = torch.matmul(prompt_norm, embedding_norm.T)
        indices = torch.argmax(similarity, dim=-1)
        discrete_prompt = embedding_matrix[indices]
        return discrete_prompt
    
    def project_soft_prompt(self, current_soft_prompt, faiss_index, embedding_space_tensor, device):
        """
        å°‡ current_soft_prompt æŠ•å½±åˆ°èªç¾©ç©ºé–“ä¸­ï¼Œç”¢ç”Ÿ projected_soft_promptï¼Œ
        åŒæ™‚ä¿ç•™æ¢¯åº¦éˆæ¥ï¼ˆè¨ˆç®—åœ–ä¸ä¸­æ–·ï¼‰
        """

        projected_soft_prompt = current_soft_prompt.clone()

        with torch.no_grad():
            query = current_soft_prompt.detach().cpu().numpy()
            _, indices = faiss_index.search(query, k=1)  # [L, 1]
            nearest_vecs = embedding_space_tensor[indices.squeeze()]  # å¯èƒ½ç‚º [d] æˆ– [L, d]

        nearest_vecs = torch.from_numpy(nearest_vecs).to(device)

        # ä¿éšªï¼šå¼·åˆ¶èˆ‡åŸ prompt åŒ shape
        if nearest_vecs.shape != current_soft_prompt.shape:
            nearest_vecs = nearest_vecs.view_as(current_soft_prompt)

        projected_soft_prompt.data = nearest_vecs

        return projected_soft_prompt
    

    def project_to_embedding_space(self):
        """
        ä½¿ç”¨æœ€è¿‘é„°æŠ•å½±å°‡é€£çºŒ Prompt åµŒå…¥æ˜ å°„åˆ°æ¨¡å‹çš„è©åµŒå…¥ç©ºé–“ã€‚
        Returns:
            torch.Tensor: æŠ•å½±å¾Œçš„åµŒå…¥å’Œæœ€è¿‘é„°ç´¢å¼•ã€‚
        """
        with torch.no_grad():
            soft_prompts = self.soft_prompts.view(-1, self.soft_prompts.shape[-1])
            soft_prompts = normalize_embeddings(soft_prompts)  # Query

            embedding_matrix = self.model.shared.weight
            embedding_matrix = normalize_embeddings(embedding_matrix)  # Corpus

            hits = semantic_search(soft_prompts, embedding_matrix, 
                                   query_chunk_size=soft_prompts.shape[0], 
                                   top_k=3, score_function=dot_score)

            nn_indices = torch.tensor([hit[0]["corpus_id"] for hit in hits], device=soft_prompts.device)
            projected_embeds = self.model.shared(nn_indices)

            return projected_embeds, nn_indices

    def integrate_projected_embeds(self):
        """
        å°‡æŠ•å½±å¾Œçš„åµŒå…¥æ•´åˆåˆ°ç•¶å‰æ¨¡å‹ä¸­ï¼Œç”¨æ–¼é€²ä¸€æ­¥è¨“ç·´æˆ–è©•ä¼°ã€‚
        """
        projected_embeds, nn_indices = self.project_to_embedding_space()
        self.soft_prompts.data = projected_embeds.data
        return nn_indices
    
    def extract_and_cache_training_sentence_embeddings(self, model, dataloader, device):
        """
        æå–ä¸¦ç·©å­˜æ•´å€‹è¨“ç·´é›†çš„å¥å­ç´šåˆ¥ä¸Šä¸‹æ–‡ç›¸é—œåµŒå…¥å‘é‡ï¼Œæ”¯æŒ 2D æ¨™ç±¤ã€‚
        Args:
            model: é è¨“ç·´çš„æ¨¡å‹ï¼Œç”¨æ–¼æå–ä¸Šä¸‹æ–‡ç›¸é—œè©åµŒå…¥ã€‚
            dataloader: åŒ…å«è¨“ç·´é›†çš„ dataloaderã€‚
            device: è¨“ç·´è¨­å‚™ï¼ˆå¦‚ "cuda" æˆ– "cpu"ï¼‰ã€‚

        Returns:
            sentence_embeddings (torch.Tensor): è¨“ç·´é›†çš„å¥å­ç´šåˆ¥åµŒå…¥ï¼Œå½¢ç‹€ç‚º [num_sentences, embedding_dim]ã€‚
            labels (torch.Tensor): è¨“ç·´é›†çš„ç›®æ¨™æ¨™ç±¤ï¼Œå½¢ç‹€ç‚º [num_sentences, num_classes]ã€‚
        """
        model.eval().to(device)
        all_sentence_embeddings = []
        all_labels = []
        all_texts = []


        with torch.no_grad():
            for batch in dataloader:
                # è¼‰å…¥æ•¸æ“š
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)  # æå–æ¨™ç±¤ï¼ˆæ”¯æŒ 2Dï¼‰
                raw_texts_batch = batch["raw_text"]

                # æå– token ç´šåˆ¥åµŒå…¥
                encoder_outputs = model.encoder(input_ids=input_ids, attention_mask=attention_mask)
                token_embeddings = encoder_outputs.last_hidden_state  # [batch_size, seq_len, embedding_dim]

                # èšåˆç‚ºå¥å­ç´šåµŒå…¥
                for i in range(token_embeddings.size(0)):  # éæ­·æ¯å€‹å¥å­
                    valid_token_mask = attention_mask[i].bool()  # éæ¿¾æœ‰æ•ˆ token
                    valid_embeddings = token_embeddings[i][valid_token_mask]  # [num_valid_tokens, embedding_dim]

                    # æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆ token
                    if valid_embeddings.size(0) == 0:
                        print(f"Warning: No valid tokens for sentence {i}. Using zero vector.")
                        sentence_embedding = torch.zeros(token_embeddings.size(-1))  # ä½¿ç”¨é›¶å‘é‡
                    else:
                        sentence_embedding = valid_embeddings.mean(dim=0)  # å¹³å‡æ± åŒ–
                    
                    all_sentence_embeddings.append(sentence_embedding.cpu())  # ä¿å­˜åµŒå…¥
                    all_texts.append(raw_texts_batch[i])

                    # ä¿å­˜ 2D æ¨™ç±¤ï¼ˆç›´æ¥ä¿ç•™åŸæ ¼å¼ï¼‰
                    all_labels.append(labels[i].cpu())  # æ³¨æ„ä¸å†è½‰æ›ç‚ºæ¨™é‡

        # ç¢ºä¿åµŒå…¥å’Œæ¨™ç±¤æ•¸é‡åŒ¹é…
        assert len(all_sentence_embeddings) == len(all_labels), "Mismatch between embeddings and labels!"
        # ç¢ºä¿åµŒå…¥èˆ‡æ–‡æœ¬æ•¸é‡åŒ¹é…
        assert len(all_sentence_embeddings) == len(all_texts), "Mismatch between embeddings and texts!"

        # å°‡æ‰€æœ‰å¥å­åµŒå…¥çµ„æˆçŸ©é™£ï¼Œå½¢ç‹€ [num_sentences, embedding_dim]
        sentence_embeddings = torch.stack(all_sentence_embeddings, dim=0)

        # å°‡æ¨™ç±¤çµ„æˆçŸ©é™£ï¼Œå½¢ç‹€ [num_sentences, num_classes]
        all_labels = torch.stack(all_labels, dim=0)

        return sentence_embeddings, all_texts

    def compute_cosine_distance_to_embedding_space(self, soft_prompt, embedding_space):
        """
        è¨ˆç®— soft prompt èˆ‡åµŒå…¥ç©ºé–“çš„å¹³å‡æœ€å° Cosine è·é›¢ã€‚

        Args:
            soft_prompt (torch.Tensor): åˆå§‹åŒ–çš„ Soft Prompt (n_tokens, embedding_dim)
            embedding_space (torch.Tensor): è¨“ç·´æ•¸æ“šæ§‹å»ºçš„åµŒå…¥ç©ºé–“ (num_samples, embedding_dim)

        Returns:
            float: Soft Prompt èˆ‡åµŒå…¥ç©ºé–“çš„å¹³å‡æœ€å° Cosine è·é›¢
        """
        soft_prompt = soft_prompt.clone().detach().cpu()
        embedding_space = torch.from_numpy(embedding_space)
        # **è¨ˆç®— Cosine Similarity**
        cosine_sim = F.cosine_similarity(soft_prompt.unsqueeze(1), embedding_space.unsqueeze(0), dim=2)  # (n_tokens, num_samples)

        # **å–æ¯å€‹ Soft Prompt å‘é‡åˆ°åµŒå…¥ç©ºé–“æœ€è¿‘çš„ Cosine Similarity**
        max_similarity, _ = cosine_sim.max(dim=1)  # æ‰¾å‡º soft prompt èˆ‡åµŒå…¥ç©ºé–“æœ€æ¥è¿‘çš„é» (å€¼è¶Šæ¥è¿‘ 1ï¼Œä»£è¡¨è¶Šç›¸ä¼¼)

        # **è¨ˆç®— Cosine è·é›¢ = 1 - æœ€å¤§ç›¸ä¼¼åº¦**
        cosine_distance = 1 - max_similarity  # Cosine Distance = 1 - Cosine Similarity

        return cosine_distance.mean().item()  # å›å‚³å¹³å‡ Cosine è·é›¢